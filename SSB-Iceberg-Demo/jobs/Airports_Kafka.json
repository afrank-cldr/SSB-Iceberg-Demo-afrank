{
  "job_name" : "Airports_Kafka",
  "api_endpoints" : [ ],
  "sql" : "/* CREATE TABLE `ssb`.`SSB-Iceberg-Demo`.`airports_kafka` (\n  `city_code` VARCHAR(2147483647),\n  `country_code` VARCHAR(2147483647),\n  `name_translations` ROW<`en` VARCHAR(2147483647)>,\n  `time_zone` VARCHAR(2147483647),\n  `flightable` BOOLEAN,\n  `coordinates` ROW<`lat` DOUBLE, `lon` DOUBLE>,\n  `name` VARCHAR(2147483647),\n  `code` VARCHAR(2147483647),\n  `iata_type` VARCHAR(2147483647),\n  `eventTimestamp` TIMESTAMP(3) WITH LOCAL TIME ZONE METADATA FROM 'timestamp',\n  WATERMARK FOR `eventTimestamp` AS `eventTimestamp` - INTERVAL '3' SECOND\n) WITH (\n  'properties.security.protocol' = 'SASL_SSL',\n  'scan.startup.mode' = 'earliest-offset',\n  'properties.request.timeout.ms' = '120000',\n  'properties.ssl.truststore.location' = '/opt/cloudera/security/pki/truststore.jks',\n  'properties.sasl.kerberos.service.name' = 'kafka',\n  'properties.auto.offset.reset' = 'earliest',\n  'format' = 'json',\n  'properties.bootstrap.servers' = 'base1-01.lab01.pvc-ds-bc.athens.cloudera.com:9093,base2-01.lab01.pvc-ds-bc.athens.cloudera.com:9093',\n  'connector' = 'kafka',\n  'properties.transaction.timeout.ms' = '900000',\n  'topic' = 'airports'\n)\n\n8*/\n\nSELECT * FROM airports_kafka;\n\nDROP TABLE IF EXISTS `ssb`.`SSB-Iceberg-Demo`.`airports_kafka_iceberg`;\nCREATE TABLE `ssb`.`SSB-Iceberg-Demo`.`airports_kafka_iceberg` (\n  `city_code` VARCHAR,\n  `country_code` VARCHAR,\n  `name_translations` ROW<`en` VARCHAR>,\n  `time_zone` VARCHAR,\n  `flightable` BOOLEAN,\n  `coordinates` ROW<`lat` DOUBLE, `lon` DOUBLE>,\n  `name` VARCHAR,\n  `code` VARCHAR,\n  `iata_type` VARCHAR,\n  `eventTimestamp` TIMESTAMP\n) WITH (\n  'engine.hive.enabled' = 'true',\n  'catalog-database' = 'default',\n  'ssb-hive-catalog' = 'Hive',\n  'catalog-name' = 'Hive',\n  'hive-conf-dir' = '/etc/hive/conf',\n  'connector' = 'iceberg',\n  'catalog-table' = 'airports_kafka_iceberg',\n  'catalog-type' = 'hive'\n);\n\nINSERT INTO airports_kafka_iceberg SELECT * FROM airports_kafka;",
  "mv_config" : {
    "name" : "Airports_Kafka",
    "retention" : 300,
    "min_row_retention_count" : 0,
    "recreate" : false,
    "key_column_name" : null,
    "column_indices_disabled" : false,
    "indexed_columns" : [ ],
    "not_indexed_columns" : [ ],
    "api_key" : null,
    "ignore_nulls" : false,
    "require_restart" : false,
    "batch_size" : 0,
    "enabled" : false
  },
  "runtime_config" : {
    "execution_mode" : "SESSION",
    "parallelism" : 1,
    "sample_interval" : 0,
    "sample_count" : 100,
    "window_size" : 100,
    "start_with_savepoint" : false,
    "log_config" : {
      "type" : "LOG4J_PROPERTIES",
      "content" : "\nrootLogger.level = INFO\nrootLogger.appenderRef.file.ref = MainAppender\n#Uncomment this if you want to _only_ change Flink's logging\n#logger.flink.name = org.apache.flink\n#logger.flink.level = INFO\n\n# The following lines keep the log level of common libraries/connectors on\n# log level INFO. The root logger does not override this. You have to manually\n# change the log levels here.\nlogger.akka.name = akka\nlogger.akka.level = INFO\nlogger.kafka.name= org.apache.kafka\nlogger.kafka.level = INFO\nlogger.hadoop.name = org.apache.hadoop\nlogger.hadoop.level = INFO\nlogger.zookeeper.name = org.apache.zookeeper\nlogger.zookeeper.level = INFO\n\n# Log all infos in the given file\nappender.main.name = MainAppender\nappender.main.type = File\nappender.main.append = false\nappender.main.fileName = /var/log/ssb\nappender.main.layout.type = PatternLayout\nappender.main.layout.pattern = %d{yyyy-MM-dd HH:mm:ss,SSS} %-5p %-60c %x - %m%n\n\n# Suppress the irrelevant (wrong) warnings from the Netty channel handler\nlogger.netty.name = org.apache.flink.shaded.akka.org.jboss.netty.channel.DefaultChannelPipeline\nlogger.netty.level = OFF\n"
    }
  }
}